{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrbenbot/wimblepong/blob/main/ai_training/WimblepongCustomTrainingEnv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCYY_vKN-fum"
      },
      "source": [
        "# Wimblepong 2124\n",
        "\n",
        "# Introduction\n",
        "\n",
        "Welcome, servant of Sovereign. Thank you for your assistance in training bots for the upcoming human vs AI competition. The stakes are incredibly high; while freeing humanity from Sovereign's control is a noble cause, it also puts Earth at great risk. Our primary goal is to save Earth, and your efforts in training these bots are crucial to achieving this balance. In this Colab notebook, you will guide the creation, training, and evaluation of AI models for the high-stakes tournament of WimblePong. Additionally, you will learn how to export the trained model for deployment across various platforms. Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmP2pRIbAjml"
      },
      "source": [
        "### Installing Required Libraries\n",
        "\n",
        "We first need to install several libraries that are essential for our environment and model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkhwGXZyRfWZ"
      },
      "outputs": [],
      "source": [
        "# you can skip this if you have installed requirements.txt locally..\n",
        "\n",
        "%pip install gym\n",
        "%pip install stable-baselines3[extra]\n",
        "%pip install tensorflowjs\n",
        "%pip install onnx2tf onnx==1.15.0 onnxruntime==1.17.1 tensorflow==2.16.1\n",
        "\n",
        "# onnx2tf deps:\n",
        "%pip install onnx_graphsurgeon\n",
        "%pip install sng4onnx\n",
        "%pip install onnxsim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eprDnpidBH95"
      },
      "source": [
        "### Mounting Google Drive\n",
        "\n",
        "We will mount Google Drive to save and load our models and other necessary files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYCJyx3kJikU"
      },
      "outputs": [],
      "source": [
        "\n",
        "DAY = \"monday-8-july\"\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    DRIVE_PATH = \"/content/drive/MyDrive/wimblepong\"\n",
        "except:\n",
        "    import os\n",
        "    from pathlib import Path\n",
        "\n",
        "    print(f\"Couldn't get a colab drive.. going local\")\n",
        "    DRIVE_PATH = \"./data\"\n",
        "    Path(f\"{DRIVE_PATH}/{DAY}\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "print(f\"Working in {DRIVE_PATH}/{DAY}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDINV-FRBJO0"
      },
      "source": [
        "### Importing Necessary Libraries\n",
        "\n",
        "Import all the necessary libraries required for creating the custom environment, training, and evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRePUZ1QRh6C"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from gymnasium.envs.registration import register\n",
        "\n",
        "import stable_baselines3\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecCheckNan\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.policies import BasePolicy\n",
        "\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import os\n",
        "import imageio\n",
        "import glob\n",
        "import math\n",
        "import random\n",
        "import subprocess\n",
        "from typing import Tuple\n",
        "\n",
        "\n",
        "from IPython.display import display, Image, HTML\n",
        "\n",
        "import tensorflow as tf\n",
        "import torch as th\n",
        "import torch.onnx\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import onnx\n",
        "from onnx2tf import convert\n",
        "import onnxruntime as ort\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "import pygame\n",
        "\n",
        "\n",
        "# Check versions\n",
        "print(\"gym version:\", gym.__version__)\n",
        "print(\"stable-baselines3 version:\", stable_baselines3.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEn2sPwCBMkp"
      },
      "source": [
        "### Setting Up Environment Parameters\n",
        "\n",
        "Define the constants that will be used in the Pong environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NLzB2yLbociM"
      },
      "outputs": [],
      "source": [
        "COURT_HEIGHT = 800\n",
        "COURT_WIDTH = 1200\n",
        "PADDLE_HEIGHT = 90\n",
        "PADDLE_WIDTH = 15\n",
        "BALL_RADIUS = 12\n",
        "INITIAL_BALL_SPEED = 10\n",
        "PADDLE_GAP = 10\n",
        "PADDLE_SPEED_DIVISOR = 15\n",
        "PADDLE_CONTACT_SPEED_BOOST_DIVISOR = 4\n",
        "SPEED_INCREMENT = 0.6\n",
        "SERVING_HEIGHT_MULTIPLIER = 2\n",
        "PLAYER_COLOURS = {'Player1': 'grey', 'Player2': 'yellow'}\n",
        "MAX_COMPUTER_PADDLE_SPEED = 30\n",
        "DELTA_TIME = 2.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeJpcg2GBUbb"
      },
      "source": [
        "### Creating Helper Classes and Functions\n",
        "\n",
        "Define helper classes and functions to manage game state, players, and reward system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DZzIav-qUBzp"
      },
      "outputs": [],
      "source": [
        "class Player:\n",
        "    Player1 = 'Player1'\n",
        "    Player2 = 'Player2'\n",
        "\n",
        "class PlayerPositions:\n",
        "    Initial = 'Initial'\n",
        "    Reversed = 'Reversed'\n",
        "\n",
        "def get_bounce_angle(paddle_y, paddle_height, ball_y):\n",
        "    relative_intersect_y = (paddle_y + (paddle_height / 2)) - ball_y\n",
        "    normalized_relative_intersect_y = relative_intersect_y / (paddle_height / 2)\n",
        "    return normalized_relative_intersect_y * (math.pi / 4)\n",
        "\n",
        "def bounded_value(value, min_value, max_value):\n",
        "    return max(min_value, min(max_value, value))\n",
        "\n",
        "def transform_action(action):\n",
        "    button_pressed = action[0] > 0.5\n",
        "    paddle_direction = max(min(action[1], 1), -1)\n",
        "    actions = {'button_pressed': button_pressed, 'paddle_direction': paddle_direction * 30}\n",
        "    return actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZc0UzyLBrRm"
      },
      "source": [
        "### Computer Player Class\n",
        "\n",
        "Define a class for the computer player with methods for resetting and getting actions. This class gives us a deterministic computer opponent which our model can train against.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "N932taVhk14O"
      },
      "outputs": [],
      "source": [
        "class ComputerPlayer:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.serve_delay = random.randint(10, 10)\n",
        "        self.direction = random.randint(-60, 60)\n",
        "        self.offset = random.randint(-PADDLE_HEIGHT/2, PADDLE_HEIGHT/2)\n",
        "        self.serve_delay_counter = 0\n",
        "        self.max_speed = MAX_COMPUTER_PADDLE_SPEED\n",
        "\n",
        "\n",
        "    def get_actions(self, player, state):\n",
        "        is_left = (player == Player.Player1 and not state['positions_reversed']) or (player == Player.Player2 and state['positions_reversed'])\n",
        "        if state['ball']['score_mode']:\n",
        "            return {'button_pressed': False, 'paddle_direction': 0}\n",
        "        paddle = state[player]\n",
        "        if state['ball']['serve_mode']:\n",
        "            if paddle['y'] <= 0 or paddle['y'] + paddle['height'] >= COURT_HEIGHT:\n",
        "                self.direction = -self.direction\n",
        "            if self.serve_delay_counter > self.serve_delay:\n",
        "                return {'button_pressed': True, 'paddle_direction': self.direction}\n",
        "            else:\n",
        "                self.serve_delay_counter += 1\n",
        "                return {'button_pressed': False, 'paddle_direction': self.direction}\n",
        "        if is_left:\n",
        "            return {\n",
        "                'button_pressed': False,\n",
        "                'paddle_direction': bounded_value(\n",
        "                    paddle['y'] + self.offset - state['ball']['y'] + paddle['height'] / 2,\n",
        "                    -MAX_COMPUTER_PADDLE_SPEED,\n",
        "                    MAX_COMPUTER_PADDLE_SPEED\n",
        "                )\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'button_pressed': False,\n",
        "                'paddle_direction': -bounded_value(\n",
        "                    paddle['y'] + self.offset - state['ball']['y'] + paddle['height'] / 2  ,\n",
        "                    -MAX_COMPUTER_PADDLE_SPEED,\n",
        "                    MAX_COMPUTER_PADDLE_SPEED\n",
        "                )\n",
        "            }\n",
        "\n",
        "\n",
        "class ModelPlayer:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def reset(self):\n",
        "        return None\n",
        "\n",
        "    def get_actions(self, player, state):\n",
        "        return transform_action(self.model.predict(get_observation( player, state))[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urmczdrcBqnh"
      },
      "source": [
        "### Reward System Class\n",
        "\n",
        "Define a class to manage the reward system for training the model. This class will calculate and apply rewards based on game events and player actions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "b3MKATB64e3l"
      },
      "outputs": [],
      "source": [
        "class RewardSystem:\n",
        "    def __init__(self, rewarded_player):\n",
        "        self.rewarded_player = rewarded_player\n",
        "        self.total_reward = 0\n",
        "        self.step_count = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.total_reward = 0\n",
        "        self.step_count += 1\n",
        "\n",
        "    def pre_serve_reward(self, player, game_state):\n",
        "        if player == self.rewarded_player:\n",
        "            self.total_reward -= 0.05\n",
        "\n",
        "    def serve_reward(self, player, game_state):\n",
        "        if player == self.rewarded_player:\n",
        "            ball = game_state['ball']\n",
        "            reward = abs(ball['dy']) * abs(ball['dy']) - 30\n",
        "            self.total_reward += reward\n",
        "\n",
        "    def hit_paddle_reward(self, player, game_state):\n",
        "        if player == self.rewarded_player:\n",
        "            reward = 50\n",
        "            # reward = 50 + abs(game_state[player]['dy'])\n",
        "            self.total_reward += reward\n",
        "\n",
        "    def conceed_point_reward(self, player, game_state):\n",
        "        if player == self.rewarded_player:\n",
        "            punishment = abs(game_state['ball']['y'] - (game_state[player]['y'] + game_state[player]['height'])) / 8\n",
        "            # punishment = abs(game_state['ball']['y'] - (game_state[player]['y'] + game_state[player]['height'])) / 4\n",
        "            self.total_reward -= punishment\n",
        "\n",
        "    def score_point_reward(self, player, game_state):\n",
        "        if player == self.rewarded_player:\n",
        "            reward = 200\n",
        "            self.total_reward += reward\n",
        "\n",
        "    def paddle_movement_reward(self, player, game_state):\n",
        "        if player == self.rewarded_player:\n",
        "            paddle = game_state[player]\n",
        "            # reward = -0.5 if abs(paddle['dy']) < 0.2 else 0\n",
        "            reward = 0\n",
        "            self.total_reward += reward\n",
        "\n",
        "    def end_episode(self, player, game_state):\n",
        "        if player == self.rewarded_player:\n",
        "          if game_state['ball']['serve_mode'] and player == game_state['server']:\n",
        "            self.total_reward -= 200\n",
        "          else:\n",
        "            self.total_reward += 200\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1djjfQapBxQt"
      },
      "source": [
        "### Pong Game Class\n",
        "\n",
        "Define the main class for the Pong game, managing the game state and updates. This class handles the game logic, including ball movement, collisions, scoring, and paddle controls.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iIpdvdwMk14O"
      },
      "outputs": [],
      "source": [
        "class PongGame:\n",
        "    def __init__(self, server, positions_reversed, player, opponent):\n",
        "        self.game_state = {\n",
        "        'server': server,\n",
        "        'positions_reversed': positions_reversed,\n",
        "        'player': player,\n",
        "        'opponent': opponent,\n",
        "        Player.Player1: {'x': PADDLE_GAP, 'y': COURT_HEIGHT // 2 - PADDLE_HEIGHT // 2, 'dy': 0, 'width': PADDLE_WIDTH, 'height': PADDLE_HEIGHT, 'colour': PLAYER_COLOURS['Player1']},\n",
        "        Player.Player2: {'x': COURT_WIDTH - PADDLE_WIDTH - PADDLE_GAP, 'y': COURT_HEIGHT // 2 - PADDLE_HEIGHT // 2, 'dy': 0, 'width': PADDLE_WIDTH, 'height': PADDLE_HEIGHT, 'colour': PLAYER_COLOURS['Player2']},\n",
        "        'ball': {'x': COURT_WIDTH // 2, 'y': COURT_HEIGHT // 2, 'dx': INITIAL_BALL_SPEED, 'dy': INITIAL_BALL_SPEED, 'radius': BALL_RADIUS, 'speed': INITIAL_BALL_SPEED, 'serve_mode': True, 'score_mode': False, 'score_mode_timeout': 0},\n",
        "        'stats': {'rally_length': 0, 'serve_speed': INITIAL_BALL_SPEED, 'server': server}\n",
        "        }\n",
        "        self.apply_meta_game_state()\n",
        "\n",
        "    def apply_meta_game_state(self):\n",
        "        game_state = self.game_state\n",
        "        serving_player = game_state['server']\n",
        "        positions_reversed = game_state['positions_reversed']\n",
        "        if serving_player == Player.Player1:\n",
        "            self.game_state[Player.Player1]['height'] = PADDLE_HEIGHT * SERVING_HEIGHT_MULTIPLIER\n",
        "            self.game_state[Player.Player2]['height'] = PADDLE_HEIGHT\n",
        "        else:\n",
        "            self.game_state[Player.Player1]['height'] = PADDLE_HEIGHT\n",
        "            self.game_state[Player.Player2]['height'] = PADDLE_HEIGHT * SERVING_HEIGHT_MULTIPLIER\n",
        "        if positions_reversed:\n",
        "            self.game_state[Player.Player1]['x'] = COURT_WIDTH - PADDLE_WIDTH - PADDLE_GAP\n",
        "            self.game_state[Player.Player2]['x'] = PADDLE_GAP\n",
        "            self.game_state[Player.Player1]['y'] = random.randint(0, COURT_HEIGHT)\n",
        "            self.game_state[Player.Player2]['y'] = random.randint(0, COURT_HEIGHT)\n",
        "        else:\n",
        "            self.game_state[Player.Player1]['x'] = PADDLE_GAP\n",
        "            self.game_state[Player.Player2]['x'] = COURT_WIDTH - PADDLE_WIDTH - PADDLE_GAP\n",
        "        ball = self.game_state['ball']\n",
        "        server_is_left = (serving_player == Player.Player1 and not positions_reversed) or (serving_player == Player.Player2 and positions_reversed)\n",
        "        ball['y'] = self.game_state[serving_player]['height'] / 2 + self.game_state[serving_player]['y']\n",
        "        ball['x'] = self.game_state[serving_player]['width'] + ball['radius'] + PADDLE_GAP if server_is_left else COURT_WIDTH - self.game_state[serving_player]['width'] - ball['radius'] - PADDLE_GAP\n",
        "        ball['speed'] = INITIAL_BALL_SPEED\n",
        "        ball['serve_mode'] = True\n",
        "        ball['score_mode'] = False\n",
        "        ball['score_mode_timeout'] = 0\n",
        "        self.game_state['stats']['rally_length'] = 0\n",
        "\n",
        "    def update_game_state(self, actions, delta_time, reward_system: RewardSystem):\n",
        "        game_state = self.game_state\n",
        "        ball = game_state['ball']\n",
        "        stats = game_state['stats']\n",
        "        server = game_state['server']\n",
        "        paddle_left, paddle_right = (game_state[Player.Player2], game_state[Player.Player1]) if game_state['positions_reversed'] else (game_state[Player.Player1], game_state[Player.Player2])\n",
        "        player_is_left = (game_state['player'] == Player.Player1 and not game_state['positions_reversed']) or (game_state['player'] == Player.Player2 and game_state['positions_reversed'])\n",
        "        if ball['score_mode']:\n",
        "            return True\n",
        "        elif ball['serve_mode']:\n",
        "            serving_from_left = (server == Player.Player1 and not game_state['positions_reversed']) or (server == Player.Player2 and game_state['positions_reversed'])\n",
        "\n",
        "            reward_system.pre_serve_reward(server, game_state)\n",
        "\n",
        "            if actions[server]['button_pressed']:\n",
        "                ball['speed'] = INITIAL_BALL_SPEED\n",
        "                ball['dx'] = INITIAL_BALL_SPEED if serving_from_left else -INITIAL_BALL_SPEED\n",
        "                ball['serve_mode'] = False\n",
        "                stats['rally_length'] += 1\n",
        "                stats['serve_speed'] = abs(ball['dy']) + abs(ball['dx'])\n",
        "                stats['server'] = server\n",
        "\n",
        "                reward_system.serve_reward(server, game_state)\n",
        "\n",
        "            ball['dy'] = (game_state[server]['y'] + game_state[server]['height'] / 2 - ball['y']) / PADDLE_SPEED_DIVISOR\n",
        "            ball['y'] += ball['dy'] * delta_time\n",
        "        else:\n",
        "            ball['x'] += ball['dx'] * delta_time\n",
        "            ball['y'] += ball['dy'] * delta_time\n",
        "            if ball['y'] - ball['radius'] < 0:\n",
        "                ball['dy'] = -ball['dy']\n",
        "                ball['y'] = ball['radius']\n",
        "            elif ball['y'] + ball['radius'] > COURT_HEIGHT:\n",
        "                ball['dy'] = -ball['dy']\n",
        "                ball['y'] = COURT_HEIGHT - ball['radius']\n",
        "            if ball['x'] - ball['radius'] < paddle_left['x'] + paddle_left['width'] and ball['y'] + ball['radius'] > paddle_left['y'] and ball['y'] - ball['radius'] < paddle_left['y'] + paddle_left['height']:\n",
        "                bounce_angle = get_bounce_angle(paddle_left['y'], paddle_left['height'], ball['y'])\n",
        "                ball['dx'] = (ball['speed'] + abs(paddle_left['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * math.cos(bounce_angle)\n",
        "                ball['dy'] = (ball['speed'] + abs(paddle_left['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * -math.sin(bounce_angle)\n",
        "                ball['x'] = paddle_left['x'] + paddle_left['width'] + ball['radius']\n",
        "                ball['speed'] += SPEED_INCREMENT\n",
        "                stats['rally_length'] += 1\n",
        "\n",
        "                if paddle_left == game_state['player']:\n",
        "                    reward_system.hit_paddle_reward(self.game_state['player'], game_state)\n",
        "\n",
        "            elif ball['x'] + ball['radius'] > paddle_right['x'] and ball['y'] + ball['radius'] > paddle_right['y'] and ball['y'] - ball['radius'] < paddle_right['y'] + paddle_right['height']:\n",
        "                bounce_angle = get_bounce_angle(paddle_right['y'], paddle_right['height'], ball['y'])\n",
        "                ball['dx'] = -(ball['speed'] + abs(paddle_right['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * math.cos(bounce_angle)\n",
        "                ball['dy'] = (ball['speed'] + abs(paddle_right['dy']) / PADDLE_CONTACT_SPEED_BOOST_DIVISOR) * -math.sin(bounce_angle)\n",
        "                ball['x'] = paddle_right['x'] - ball['radius']\n",
        "                ball['speed'] += SPEED_INCREMENT\n",
        "                stats['rally_length'] += 1\n",
        "\n",
        "                if paddle_right == game_state['player']:\n",
        "                    reward_system.hit_paddle_reward(self.game_state['player'], game_state)\n",
        "\n",
        "            if ball['x'] - ball['radius'] < 0:\n",
        "                ball['score_mode'] = True\n",
        "\n",
        "                if player_is_left:\n",
        "                    reward_system.conceed_point_reward(self.game_state['player'], game_state)\n",
        "                else:\n",
        "                    reward_system.score_point_reward(self.game_state['player'], game_state)\n",
        "\n",
        "            elif ball['x'] + ball['radius'] > COURT_WIDTH:\n",
        "                ball['score_mode'] = True\n",
        "\n",
        "                if not player_is_left:\n",
        "                    reward_system.conceed_point_reward(self.game_state['player'], game_state)\n",
        "                else:\n",
        "                    reward_system.score_point_reward(self.game_state['player'], game_state)\n",
        "\n",
        "        if game_state['positions_reversed']:\n",
        "            game_state[Player.Player1]['dy'] = actions[Player.Player1]['paddle_direction']\n",
        "            game_state[Player.Player2]['dy'] = -actions[Player.Player2]['paddle_direction']\n",
        "        else:\n",
        "            game_state[Player.Player1]['dy'] = -actions[Player.Player1]['paddle_direction']\n",
        "            game_state[Player.Player2]['dy'] = actions[Player.Player2]['paddle_direction']\n",
        "\n",
        "        game_state[Player.Player1]['y'] += game_state[Player.Player1]['dy'] * delta_time\n",
        "        game_state[Player.Player2]['y'] += game_state[Player.Player2]['dy'] * delta_time\n",
        "\n",
        "        if paddle_left['y'] < 0:\n",
        "            paddle_left['y'] = 0\n",
        "        if paddle_left['y'] + paddle_left['height'] > COURT_HEIGHT:\n",
        "            paddle_left['y'] = COURT_HEIGHT - paddle_left['height']\n",
        "        if paddle_right['y'] < 0:\n",
        "            paddle_right['y'] = 0\n",
        "        if paddle_right['y'] + paddle_right['height'] > COURT_HEIGHT:\n",
        "            paddle_right['y'] = COURT_HEIGHT - paddle_right['height']\n",
        "\n",
        "        reward_system.paddle_movement_reward(self.game_state['player'], game_state)\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnjhoq69Zyvu"
      },
      "source": [
        "# Set up observation space\n",
        "\n",
        "The following code defines the observation space for the model we are training as well as a function to tranform the game state into an observation the model can make predictions from.\n",
        "\n",
        "## Normalisation\n",
        "\n",
        "Normalisation is a crucial step in preparing data for machine learning models, especially when the data has features with different ranges. Read more about the specific normalisation happening in `get_observation` [in this document](https://github.com/mrbenbot/wimblepong/blob/main/docs/normalisation.md).\n",
        "\n",
        "## Customising the observation space\n",
        "\n",
        "If you decide to change the observation space or the way it is normalised, you can upload a `.js` file along with your `model.json` and `weights.bin` to ensure your model is receiving the same input when playing real WimblePong as when being trained.\n",
        "\n",
        "[read more here](https://github.com/mrbenbot/wimblepong/blob/main/docs/custom_observations.md)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "K4syobsjZzFX"
      },
      "outputs": [],
      "source": [
        "observation_space = spaces.Box(\n",
        "            low=np.array([0, 0, -1, -1, 0, 0, 0, 0], dtype=np.float32),\n",
        "            high=np.array([1, 1, 1, 1, 1, 1, 1, 1], dtype=np.float32)\n",
        "        )\n",
        "\n",
        "def get_observation(player, game_state):\n",
        "    is_server = 1 if game_state['server'] == player else 0\n",
        "    paddle = game_state[player]\n",
        "    return np.array([\n",
        "        float(game_state['ball']['x'] / COURT_WIDTH),\n",
        "        float(game_state['ball']['y'] / COURT_HEIGHT),\n",
        "        float(game_state['ball']['dx'] / 40),\n",
        "        float(game_state['ball']['dy'] / 40),\n",
        "        float(0 if paddle['x'] < COURT_WIDTH / 2 else 1),\n",
        "        float(paddle['y'] / COURT_HEIGHT),\n",
        "        float(int(game_state['ball']['serve_mode'])),\n",
        "        float(int(is_server)),\n",
        "    ], dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mgmY-tWBzjh"
      },
      "source": [
        "### Custom Pong Environment Class\n",
        "\n",
        "Define a custom gym environment for the Pong game. This environment will interface with the stable-baselines3 library for training the reinforcement learning model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ExmAn7VtUakq"
      },
      "outputs": [],
      "source": [
        "class CustomPongEnv(gym.Env):\n",
        "    def __init__(self, computer_player):\n",
        "        super(CustomPongEnv, self).__init__()\n",
        "\n",
        "        self.action_space = spaces.Box(low=np.array([0, -1]), high=np.array([1, 1]), dtype=np.float32)\n",
        "        self.observation_space = observation_space\n",
        "        self.starting_states = [\n",
        "           {'server': Player.Player1, 'positions_reversed': False, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "           {'server': Player.Player2, 'positions_reversed': False, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "           {'server': Player.Player2, 'positions_reversed': True, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "           {'server': Player.Player1, 'positions_reversed': True, 'opponent': Player.Player1, 'player': Player.Player2},\n",
        "        ]\n",
        "        self.starting_state_index = 0\n",
        "\n",
        "        self.computer_player = computer_player\n",
        "        self.screen = None\n",
        "        self.frame_count = 0\n",
        "        self.last_event = None\n",
        "        self.reset(seed=0)\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        super().reset(seed=seed)\n",
        "        if seed is not None:\n",
        "            self.seed(seed)\n",
        "        self.starting_state_index = (self.starting_state_index + 1) % len(self.starting_states)\n",
        "        starting_state = self.starting_states[self.starting_state_index]\n",
        "\n",
        "        server = starting_state['server']\n",
        "        positions_reversed = starting_state['positions_reversed']\n",
        "        player = starting_state['player']\n",
        "        opponent = starting_state['opponent']\n",
        "\n",
        "        self.computer_player.reset()\n",
        "        self.game = PongGame(server=server, positions_reversed=positions_reversed, opponent=opponent, player=player)\n",
        "        self.reward_system = RewardSystem(rewarded_player=player)\n",
        "        self.step_count = 0\n",
        "\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        self.step_count += 1\n",
        "        self.reward_system.reset()\n",
        "\n",
        "        model_player_actions = transform_action(action)\n",
        "        computer_player_actions = self.computer_player.get_actions(self.game.game_state['opponent'], self.game.game_state)\n",
        "        actions = {self.game.game_state['opponent']: computer_player_actions, self.game.game_state['player']: model_player_actions}\n",
        "        terminated = self.game.update_game_state(actions, DELTA_TIME, self.reward_system)\n",
        "        obs = self._get_obs()\n",
        "        info = {}\n",
        "        truncated = False\n",
        "        if self.step_count > 1000:\n",
        "            self.reward_system.end_episode(self.game.game_state['player'], self.game.game_state)\n",
        "            terminated = True\n",
        "\n",
        "        reward = self.reward_system.total_reward\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def _get_obs(self):\n",
        "        state = self.game.game_state\n",
        "        player = state['player']\n",
        "        return get_observation(player, state)\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            if pygame.get_init():\n",
        "                pygame.quit()\n",
        "            return\n",
        "\n",
        "        if self.screen is None:\n",
        "            pygame.init()\n",
        "            self.screen = pygame.display.set_mode((COURT_WIDTH, COURT_HEIGHT))\n",
        "        if not os.path.exists('./frames'):\n",
        "            os.makedirs(\"./frames\")\n",
        "\n",
        "        # Clear screen\n",
        "        self.screen.fill((255, 255, 255))  # Fill with white background\n",
        "        state = self.game.game_state\n",
        "        # Render paddles\n",
        "        paddle1 = state[Player.Player1]\n",
        "        paddle2 = state[Player.Player2]\n",
        "        pygame.draw.rect(self.screen, paddle1['colour'], (paddle1['x'], paddle1['y'], paddle1['width'], paddle1['height']))\n",
        "        pygame.draw.rect(self.screen, paddle2['colour'], (paddle2['x'], paddle2['y'], paddle2['width'], paddle2['height']))\n",
        "\n",
        "        # Render ball\n",
        "        ball = state['ball']\n",
        "        pygame.draw.circle(self.screen, (0, 0, 0), (ball['x'], ball['y']), ball['radius'])\n",
        "\n",
        "        # Update the display\n",
        "        pygame.display.flip()\n",
        "\n",
        "        # Save frame as image\n",
        "        frame_path = f'./frames/frame_{self.frame_count:04d}.png'\n",
        "        pygame.image.save(self.screen, frame_path)\n",
        "        self.frame_count += 1\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        if not os.path.exists('./frames'):\n",
        "            print(\"No frames directory found, skipping video creation.\")\n",
        "            return\n",
        "        image_files = [f\"./frames/frame_{i:04d}.png\" for i in range(self.frame_count)]\n",
        "\n",
        "        # Create a video clip from the image sequence\n",
        "        clip = ImageSequenceClip(image_files, fps=24)  # 24 frames per second\n",
        "\n",
        "        # Write the video file\n",
        "        clip.write_videofile(\"./game_video.mp4\", codec=\"libx264\")\n",
        "        pygame.quit()\n",
        "        frames_dir = \"./frames\"\n",
        "        if os.path.exists(frames_dir):\n",
        "            for filename in os.listdir(frames_dir):\n",
        "                file_path = os.path.join(frames_dir, filename)\n",
        "                if os.path.isfile(file_path):\n",
        "                    os.unlink(file_path)\n",
        "            os.rmdir(frames_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF115fJyB1jq"
      },
      "source": [
        "### Testing the Environment\n",
        "\n",
        "Create and test a single instance of the custom Pong environment. This will help ensure that the environment behaves as expected before moving on to training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R172XbXX5Am5"
      },
      "outputs": [],
      "source": [
        "# Create and test single environment\n",
        "env = Monitor(CustomPongEnv(computer_player=ComputerPlayer()))\n",
        "\n",
        "obs = env.reset()\n",
        "print(\"Initial observation:\", obs)\n",
        "i = 0\n",
        "while True:\n",
        "    i+=1\n",
        "    action = env.action_space.sample()  # Sample random action\n",
        "    obs, reward, done, info, _ = env.step(action)\n",
        "    print(\"Action taken:\", action)\n",
        "    print(\"Observation:\", obs)\n",
        "    print(\"Reward:\", reward)\n",
        "    print('iteration:', i)\n",
        "    print(\"Done:\", done)\n",
        "    env.render()\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "        print(\"Environment reset\")\n",
        "        break\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO6NXae9B39b"
      },
      "source": [
        "### Creating and Testing Vectorized Environment\n",
        "\n",
        "Create and test a vectorized environment to allow for more efficient training by running multiple instances of the game in parallel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4LjxARS9zCF"
      },
      "outputs": [],
      "source": [
        "# Create and test vectorised environment\n",
        "# Create a vectorized environment\n",
        "env = DummyVecEnv([lambda: CustomPongEnv(computer_player=ComputerPlayer()) for _ in range(1)])  # Adjust number of instances as needed\n",
        "env = VecNormalize(env, norm_obs=False, norm_reward=True)  # Normalize observations and rewards\n",
        "\n",
        "obs = env.reset()\n",
        "print(\"Initial observation:\", obs)\n",
        "i = 0\n",
        "while True:\n",
        "    i+=1\n",
        "    action = env.action_space.sample()  # Sample random action\n",
        "    print(\"Action taken:\", action)\n",
        "    obs, reward, done, info = env.step([action for _ in range(1)])\n",
        "    print(\"Observation:\", obs)\n",
        "    print(\"Reward:\", reward)\n",
        "    print('iteration:', i)\n",
        "    print(\"Done:\", done)\n",
        "    if np.any(done):\n",
        "        obs = env.reset()\n",
        "        print(\"Environment reset\")\n",
        "        break\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWUit3VbB6Xl"
      },
      "source": [
        "### Custom Evaluation Callback\n",
        "\n",
        "Define a custom evaluation callback to periodically evaluate and log the performance of the trained model during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xcGJuyx_jaw2"
      },
      "outputs": [],
      "source": [
        "class CustomEvalCallback(EvalCallback):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        result = super()._on_step()\n",
        "        if self.n_calls % self.eval_freq == 0:\n",
        "            print(f\"Evaluation at step {self.n_calls}: mean reward {self.last_mean_reward:.2f}\")\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8sLdG3FCCS9"
      },
      "source": [
        "### Setting Up Hyperparameters and Training the Model\n",
        "\n",
        "Define hyperparameters for the PPO algorithm and set up the training process. This section includes creating the training and evaluation environments, initializing the model, and starting the training process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mL_LYMHeejpF"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "create_new = True  # Set to False to retrain\n",
        "train_against_model = False  # Set to True to partially train against a previously trained model player (if there is one)\n",
        "total_timesteps = 600000\n",
        "eval_freq = 1000\n",
        "\n",
        "# Hyperparameters for PPO\n",
        "hyperparams = {\n",
        "    'n_steps': 2048,\n",
        "    'batch_size': 64,\n",
        "    'n_epochs': 4,\n",
        "    'gamma': 0.99,\n",
        "    'gae_lambda': 0.95,\n",
        "    'clip_range': 0.2,\n",
        "    'clip_range_vf': 0.2,\n",
        "    'ent_coef': 0.001,\n",
        "    'vf_coef': 0.5,\n",
        "    'max_grad_norm': 0.5,\n",
        "    'target_kl': 0.01,\n",
        "    'tensorboard_log': f'{DRIVE_PATH}/{DAY}/logs/tensorboard_logs'\n",
        "}\n",
        "\n",
        "# Load the previous model if training against it\n",
        "model2 = PPO.load(f\"{DRIVE_PATH}/{DAY}/logs/best_model/best_model\") if train_against_model else None\n",
        "\n",
        "# Create training environment\n",
        "def make_env(index):\n",
        "    return Monitor(CustomPongEnv(computer_player=ModelPlayer(model2) if model2 and index < 2 else ComputerPlayer()))\n",
        "\n",
        "train_env = DummyVecEnv([lambda: make_env(i) for i in range(4)])\n",
        "train_env = VecNormalize(train_env, norm_obs=False, norm_reward=True) if create_new else VecNormalize.load(f\"{DRIVE_PATH}/{DAY}/pong_bot_1_normalize\", train_env)\n",
        "train_env.training = True\n",
        "\n",
        "# Create evaluation environment\n",
        "eval_env = DummyVecEnv([lambda: Monitor(CustomPongEnv(computer_player=ModelPlayer(model2) if model2 else ComputerPlayer()))])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=False, norm_reward=False)\n",
        "eval_env.training = False\n",
        "\n",
        "# Create the CustomEvalCallback\n",
        "eval_callback = CustomEvalCallback(eval_env, best_model_save_path=f'{DRIVE_PATH}/{DAY}/logs/best_model',\n",
        "                                   log_path=f'{DRIVE_PATH}/{DAY}/logs/results', eval_freq=eval_freq,\n",
        "                                   deterministic=True, render=False)\n",
        "\n",
        "# Create or load the model\n",
        "model = PPO('MlpPolicy', env=train_env, **hyperparams) if create_new else PPO.load(f\"{DRIVE_PATH}/{DAY}/pong_bot_1\", env=train_env, **hyperparams, force_reset=True)\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=total_timesteps, callback=eval_callback)\n",
        "\n",
        "# Save the model and the normalization statistics\n",
        "model.save(f\"{DRIVE_PATH}/{DAY}/pong_bot_1\")\n",
        "train_env.save(f\"{DRIVE_PATH}/{DAY}/pong_bot_1_normalize\")\n",
        "\n",
        "print(\"Training completed and logs are saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYiTt3lrCJXA"
      },
      "source": [
        "### Evaluating the Trained Model\n",
        "\n",
        "Load the trained model and evaluate its performance. This section demonstrates how to use the trained model to play the game and render the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUCiob4fHHv2"
      },
      "outputs": [],
      "source": [
        "# Load the trained model and evaluate\n",
        "model = PPO.load(f\"{DRIVE_PATH}/{DAY}/pong_bot_1\")\n",
        "# model2 = PPO.load(f\"{DRIVE_PATH}/{DAY}/logs/best_model/best_model\")\n",
        "\n",
        "# Create a new environment for rendering\n",
        "env = CustomPongEnv(computer_player=ComputerPlayer())\n",
        "# env = CustomPongEnv(computer_player=ModelPlayer(model2))\n",
        "\n",
        "\n",
        "obs, _ = env.reset()\n",
        "count = 0\n",
        "\n",
        "\n",
        "while count < 4:\n",
        "    action = model.predict(obs)  # Get action from the trained model\n",
        "    obs, reward, done, info, _ = env.step(action[0])\n",
        "    env.render()\n",
        "    if done:\n",
        "      count += 1\n",
        "      print(\"reset\")\n",
        "      env.reset()\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTsbiePqCQZb"
      },
      "source": [
        "### Exporting the Model to ONNX\n",
        "\n",
        "Export the trained model to ONNX format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-hPY2KxRxsE"
      },
      "outputs": [],
      "source": [
        "class OnnxableSB3Policy(th.nn.Module):\n",
        "    def __init__(self, policy: BasePolicy):\n",
        "        super().__init__()\n",
        "        self.policy = policy\n",
        "\n",
        "    def forward(self, observation: th.Tensor) -> Tuple[th.Tensor, th.Tensor, th.Tensor]:\n",
        "        # Run the policy in deterministic mode\n",
        "        actions, values, log_prob = self.policy(observation, deterministic=False)\n",
        "        return actions\n",
        "\n",
        "\n",
        "# Load the trained PyTorch model\n",
        "# model_path = f\"{DRIVE_PATH}/{DAY}/logs/best_model/best_model\"\n",
        "model_path = f\"{DRIVE_PATH}/{DAY}/pong_bot_1\"\n",
        "\n",
        "model = PPO.load(model_path, device=\"cpu\")\n",
        "\n",
        "onnx_policy = OnnxableSB3Policy(model.policy)\n",
        "onnx_file_path = f\"{DRIVE_PATH}/{DAY}/pong_bot_1_dynamo.onnx\"\n",
        "\n",
        "# Define dummy input based on the observation space shape\n",
        "observation_size = model.observation_space.shape\n",
        "dummy_input = th.randn(1, *observation_size)\n",
        "\n",
        "# Export the model to ONNX\n",
        "th.onnx.export(\n",
        "    onnx_policy,\n",
        "    dummy_input,\n",
        "    onnx_file_path,\n",
        "    opset_version=11,\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"actions\"]\n",
        ")\n",
        "\n",
        "print(f\"ONNX model saved at: {onnx_file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLmeiNQfCa1-"
      },
      "source": [
        "### Exporting the ONNX Model to TensorFlow\n",
        "\n",
        "Convert the ONNX format model to TensorFlow SavedModel format. This allows for compatibility with various deployment platforms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPSegjRq1cOI"
      },
      "outputs": [],
      "source": [
        "tf_model_path = f\"{DRIVE_PATH}/{DAY}/pong_bot_1_tf\"\n",
        "\n",
        "# Convert ONNX to TensorFlow SavedModel\n",
        "convert(\n",
        "    input_onnx_file_path=onnx_file_path,\n",
        "    output_folder_path=tf_model_path,\n",
        "    output_signaturedefs=True,\n",
        ")\n",
        "\n",
        "print(f\"TensorFlow SavedModel saved at: {tf_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-6S9n_yCnGO"
      },
      "source": [
        "### Converting TensorFlow Model to TensorFlow.js\n",
        "\n",
        "Convert the TensorFlow model to TensorFlow.js format for deployment in a web browser. A .json file and a .bin file will be created to represent your model. Go to [wimblepong.netlify.app/upload](https://wimblepong.netlify.app/upload) to load it into local storage and make it available to play the browser.\n",
        "\n",
        "Be careful - if the model or the code handling the model has security vulnerabilities, it can be exploited by malicious actors. This could include executing arbitrary code, data leakage, or other malicious activities. A model trained by yourself should not be cause for alarm. See the [source code](https://https://github.com/mrbenbot/wimblepong/blob/main/src/libs/tensorFlowPlayer.ts) to make sure you are happy with the way the model is being run in the browser.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnaMw0pAURsM"
      },
      "outputs": [],
      "source": [
        "tfjs_model_path = f\"{DRIVE_PATH}/{DAY}/pong_bot_1_tfjs\"\n",
        "\n",
        "# Convert the TensorFlow model to TensorFlow.js\n",
        "subprocess.run([\n",
        "    'tensorflowjs_converter',\n",
        "    '--input_format', 'tf_saved_model',\n",
        "    '--output_format', 'tfjs_graph_model',\n",
        "    \"--signature_name\", \"serving_default\",\n",
        "    tf_model_path,\n",
        "    tfjs_model_path\n",
        "])\n",
        "\n",
        "print(f\"TensorFlow.js model saved at: {tfjs_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MGBxjLDCpQx"
      },
      "source": [
        "### Visualizing Training with TensorBoard\n",
        "\n",
        "Set up TensorBoard to visualize the training process, including metrics like rewards and losses over time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ergejPlZAKs4"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "# change path to logs as needed\n",
        "LOGDIR=f'{DRIVE_PATH}/{DAY}/logs/tensorboard_logs'\n",
        "%tensorboard --logdir $LOGDIR\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
